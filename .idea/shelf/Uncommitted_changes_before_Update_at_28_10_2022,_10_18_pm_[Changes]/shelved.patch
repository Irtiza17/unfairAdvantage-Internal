Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import csv\nimport copy\nimport itertools\nimport depthai\nimport pandas as pd\nimport cv2 as cv\nimport numpy as np\nimport mediapipe as mp\nfrom model import KeyPointClassifier\nfrom scoring import score,secondScore, videoMapping\nimport datetime\nimport subprocess\nimport sys\n\n\ncamera = \"cam\"\ntotal_video_dur = 30\nshow_live = True\nvideo_display = False\nvideo_file_name = \"demovideo.mp4\"\ncamera_to_use = 1\n\ndef main():\n\n    HeadModel = True\n    focusModel = True\n    emotionModel = True\n\n    #Camera Values can be \"cam\",\"video\",\"oakD\" depending on income stream source.\n\n\n    model_path ='model/keypoint_classifier/keypoint_classifier.tflite' # Focus Model Path \n    model_path2 ='model/keypoint_classifier/keypoint_classifier2.tflite' # Emotion Model Path\n    model_path3 ='model/keypoint_classifier/keypoint_classifier3.tflite' # Head Model\n\n    ROI = [246, 161, 160, 159, 158, 157, 173, 33, 7, 163, 144, 145, 153, 154, 155, 133, 473, 474, 475, 476, \n            477, 466, 388, 387, 386, 385, 384, 398, 263, 249, 390, 373, 374, 380, 381, 382, 362, 468,469, 470, \n            471, 472]\n\n    noseTip= [1]\n    noseBottom= [2]\n    noseRightCorner= [98]\n    noseLeftCorner= [327]\n\n    rightCheek= [205]\n    leftCheek= [425]\n\n    silhouette= [\n    10,  338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,\n    397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,\n    172, 58,  132, 93,  234, 127, 162, 21,  54,  103, 67,  109\n    ]\n\n    ROI2 =  silhouette + noseTip + noseBottom + noseRightCorner + noseLeftCorner + rightCheek + leftCheek\n\n    filepath = 'scorelog/score.csv' #csv file for score data of every frame.\n    filepath2 = 'scorelog/secondscore.csv' # csv file for score data of every second. \n    filepath3 = 'scorelog/VideoScore.csv' # csv file for videos information and score mapping against videos.\n\n    df = pd.DataFrame(columns=['Date','Time','Focus','Emotion','Head'])\n\n    def calc_landmark_list(image, landmarks,ROI=False):\n        image_width, image_height = image.shape[1], image.shape[0]\n\n        landmark_point = []\n        # Keypoint\n        if ROI == False:\n            for _, landmark in enumerate(landmarks.landmark):\n            # for i in ROI:\n                # landmark = landmarks.landmark[i]\n                landmark_x = min(int(landmark.x * image_width), image_width - 1)\n                landmark_y = min(int(landmark.y * image_height), image_height - 1)\n                landmark_point.append([landmark_x, landmark_y])\n\n        elif ROI != False:\n            for i in ROI:\n                landmark = landmarks.landmark[i]\n                landmark_x = min(int(landmark.x * image_width), image_width - 1)\n                landmark_y = min(int(landmark.y * image_height), image_height - 1)\n                landmark_point.append([landmark_x, landmark_y])\n\n        return landmark_point\n\n\n    def pre_process_landmark(landmark_list):\n        temp_landmark_list = copy.deepcopy(landmark_list)\n\n        # Convert to relative coordinates\n        base_x, base_y = 0, 0\n        for index, landmark_point in enumerate(temp_landmark_list):\n            if index == 0:\n                base_x, base_y = landmark_point[0], landmark_point[1]\n\n            temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n            temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n\n        # Convert to a one-dimensional list\n        temp_landmark_list = list(\n            itertools.chain.from_iterable(temp_landmark_list))\n\n        # Normalization\n        max_value = max(list(map(abs, temp_landmark_list)))\n\n        def normalize_(n):\n            return n / max_value\n\n        temp_landmark_list = list(map(normalize_, temp_landmark_list))\n\n        return temp_landmark_list\n\n\n    def draw_bounding_rect(use_brect, image, brect):\n        if use_brect:\n            # Outer rectangle\n            cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n                        (0, 0, 0), 1)\n\n        return image\n\n\n    def calc_bounding_rect(image, landmarks):\n        image_width, image_height = image.shape[1], image.shape[0]\n\n        landmark_array = np.empty((0, 2), int)\n\n        for _, landmark in enumerate(landmarks.landmark):\n            landmark_x = min(int(landmark.x * image_width), image_width - 1)\n            landmark_y = min(int(landmark.y * image_height), image_height - 1)\n\n            landmark_point = [np.array((landmark_x, landmark_y))]\n            # print(\"landmark Point is \" , landmark_point)\n\n            landmark_array = np.append(landmark_array, landmark_point, axis=0)\n            # print(\"landmark array is \" , landmark_array)\n\n        x, y, w, h = cv.boundingRect(landmark_array)\n\n        return [x, y, x + w, y + h]\n\n\n    def draw_info_text(image, focus_text,emotion_text='',head_text=''):\n        cv.rectangle(image, (0, 0), (250,160),\n                    (0, 0, 0), -1)\n\n        if focus_text != \"\":\n            info_text = 'Gaze: ' + focus_text\n            cv.putText(image, info_text, (5,30),\n                    cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 1, cv.LINE_AA)\n        \n        if emotion_text != \"\":\n            info_text2 = 'Emotion: ' + emotion_text\n            cv.putText(image, info_text2, (5,70),\n                    cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 1, cv.LINE_AA)\n                    \n        if head_text != \"\":\n            info_text3 = 'Head: ' + head_text\n            cv.putText(image, info_text3, (5,110),\n                    cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 1, cv.LINE_AA)\n        val = []\n        if focus_text == 'Focused':\n            val.append(1)\n        if emotion_text == 'Positive':\n            val.append(1)\n        if head_text == 'Center':\n            val.append(1)\n        if HeadModel == False:\n            pars = 2\n        else:\n            pars = 3\n\n        cv.putText(image, f\"Score: {sum(val)}/{pars}\", (5,150),\n                    cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 1, cv.LINE_AA)\n\n        return image\n\n\n    def oakD():\n        pipeline = depthai.Pipeline()\n        cam_rgb = pipeline.createColorCamera()\n        cam_rgb.setPreviewSize(300, 300) \n        cam_rgb.setInterleaved(False)\n        xout_rgb = pipeline.createXLinkOut()\n        xout_rgb.setStreamName(\"rgb\")\n        cam_rgb.preview.link(xout_rgb.input)\n        return pipeline\n\n        \n    mode = 0\n\n    # Model load\n    mp_face_mesh = mp.solutions.face_mesh\n    face_mesh = mp_face_mesh.FaceMesh(\n            max_num_faces=1,\n            refine_landmarks=True,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5) \n\n    keypoint_classifier = KeyPointClassifier(model_path)\n    keypoint_classifier2 = KeyPointClassifier(model_path2)\n    keypoint_classifier3 = KeyPointClassifier(model_path3)\n\n    with open('model/keypoint_classifier/keypoint_classifier_label.csv',\n                encoding='utf-8-sig') as f:\n        keypoint_classifier_labels = csv.reader(f)\n        keypoint_classifier_labels = [\n            row[0] for row in keypoint_classifier_labels\n        ]\n\n    # Read labels\n    with open('model/keypoint_classifier/keypoint_classifier_label2.csv',\n                encoding='utf-8-sig') as f:\n        keypoint_classifier_labels2 = csv.reader(f)\n        keypoint_classifier_labels2 = [\n            row[0] for row in keypoint_classifier_labels2\n        ]\n\n    with open('model/keypoint_classifier/keypoint_classifier_label3.csv',\n                encoding='utf-8-sig') as f:\n        keypoint_classifier_labels3 = csv.reader(f)\n        keypoint_classifier_labels3 = [\n            row[0] for row in keypoint_classifier_labels3\n        ]\n\n    use_brect = True    \n\n    if camera == 'cam':\n        cap_device = camera_to_use\n        cap_width = 1920\n        cap_height = 1080\n        # Camera preparation\n        cap = cv.VideoCapture(cap_device)\n        cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n        cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n\n    elif camera == 'oakD':\n        pipeline= oakD()\n        with depthai.Device(pipeline) as device:\n            q_rgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n            while True:\n            # Process Key (ESC: end)\n                key = cv.waitKey(10)\n                if key == 27:  # ESC\n                    break\n                # Camera capture\n                in_rgb = q_rgb.get()\n                image = in_rgb.getCvFrame()\n\n    elif camera == 'video':\n        cap_device = 'videos/video1.mp4'\n        cap_width = 1920\n        cap_height = 1080\n        # Camera preparation\n        cap = cv.VideoCapture(cap_device)\n        cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n        cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n\n\n    start = 0 # While start = 0, it will continue to provide stream, at decided time (21s), it will stop the stream by changing its value.\n\n    if video_display:\n\n        systemOS = get_OS_platform()\n\n        if systemOS == \"Windows\":\n            child_process = subprocess.Popen([\"C:/Program Files/VideoLAN/VLC/vlc.exe\",\"videos/\" + video_file_name])\n            print(\"Windows\")\n        elif systemOS == \"OS X\":\n            child_process = subprocess.Popen([\"/Applications/VLC.app/contents/MacOS/vlc\", \"videos/\" + video_file_name])\n            print(\"Mac OS\")\n        else: print(\"Other OS\")\n\n    now = datetime.datetime.now()\n\n\n    while start == 0:\n        next = datetime.datetime.now()\n        # print('Next1',next)\n        timedif = (next - now).seconds\n        # Process Key (ESC: end)\n        key = cv.waitKey(10)\n        if key == 27:  # ESC\n            break\n        if timedif > total_video_dur:\n            start = 1\n        # Camera capture\n        ret, image = cap.read()\n        if not ret:\n            break\n        \n        image = cv.flip(image, 1)  # Mirror display\n        debug_image = copy.deepcopy(image)\n\n        # Detection implementation\n        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n\n        image.flags.writeable = False\n        results = face_mesh.process(image)\n        image.flags.writeable = True\n\n        if results.multi_face_landmarks is not None:\n            for face_landmarks in results.multi_face_landmarks:\n                # Bounding box calculation\n                brect = calc_bounding_rect(debug_image, face_landmarks)\n\n                # Landmark calculation\n                if focusModel == True:\n                    focus_landmark_list = calc_landmark_list(debug_image, face_landmarks,ROI)\n                    # Conversion to relative coordinates / normalized coordinates\n                    pre_processed_landmark_list1 = pre_process_landmark(\n                    focus_landmark_list)\n                    facial_focus_id = keypoint_classifier(pre_processed_landmark_list1)\n\n                if emotionModel == True:\n                    emotion_landmark_list = calc_landmark_list(debug_image, face_landmarks)\n                    # Conversion to relative coordinates / normalized coordinates\n                    pre_processed_landmark_list2 = pre_process_landmark(\n                    emotion_landmark_list)\n                    facial_emotion_id = keypoint_classifier2(pre_processed_landmark_list2)\n\n                if HeadModel == True:\n                    head_landmark_list = calc_landmark_list(debug_image, face_landmarks,ROI2)\n                    # Conversion to relative coordinates / normalized coordinates\n                    pre_processed_landmark_list3 = pre_process_landmark(\n                    head_landmark_list)\n                    head_id = keypoint_classifier3(pre_processed_landmark_list3)\n\n            # Drawing part\n            try:\n                debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n                debug_image = draw_info_text(\n                                    debug_image,\n                                    keypoint_classifier_labels[facial_focus_id],keypoint_classifier_labels2[facial_emotion_id],keypoint_classifier_labels3[head_id]\n                                    )\n                # Scoring part\n                eyeFocusVal = keypoint_classifier_labels[facial_focus_id]\n                emotionVal = keypoint_classifier_labels2[facial_emotion_id]\n                headVal = keypoint_classifier_labels3[head_id]\n                df = score(eyeFocusVal,emotionVal,headVal,next,df)\n            except:\n                pass\n\n        else:\n            # Scoring part\n            eyeFocusVal = 'Not Focused'\n            emotionVal = 'Negative'\n            headVal = 'Not Center'\n            df = score(eyeFocusVal,emotionVal,headVal,next,df)\n            cv.rectangle(debug_image, (0, 0), (1920,100),\n                    (0, 0,255), -1)\n            cv.putText(debug_image, \"CANNOT DETECT FACE\", (300,75),\n                    cv.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv.LINE_AA)\n\n        # Screen reflection\n        if show_live:\n            cv.imshow('Facial Emotion and focus Recognition', debug_image)\n        # if timedif >= 10 and ret2:\n\n\n    # df.to_csv(filepath,index=False)\n    # df2 = secondScore(df)\n    # df2.to_csv(filepath2,index=False)\n    # df3 = videoMapping(df2)\n    # df3.to_csv(filepath3,index=False)\n    # # cap.release()\n    # cv.destroyAllWindows()\n    # child_process.terminate()\n\n\ndef get_OS_platform():\n    platforms = {\n        'linux' : 'Linux',\n        'darwin' : 'OS X',\n        'win32' : 'Windows',\n        'nt' : 'Windows',\n        'win64' : 'Windows'\n    }\n    if sys.platform not in platforms:\n        return sys.platform\n\n    return platforms[sys.platform]\n\nif __name__ == \"__main__\":\n    main()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 3aed6a2e3b78487eaf723510d0460f2297cae170)
+++ b/main.py	(date 1666976998120)
@@ -14,11 +14,11 @@
 
 
 camera = "cam"
-total_video_dur = 30
+total_video_dur = 200
 show_live = True
-video_display = False
+video_display = True
 video_file_name = "demovideo.mp4"
-camera_to_use = 1
+camera_to_use = 0
 
 def main():
 
